{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdNB3HyFuGWN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uliWZAv4yJ2",
        "outputId": "11bb957c-044f-488a-c953-77575b19c9e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'BELab'...\n",
            "remote: Enumerating objects: 808, done.\u001b[K\n",
            "remote: Counting objects: 100% (808/808), done.\u001b[K\n",
            "remote: Compressing objects: 100% (804/804), done.\u001b[K\n",
            "remote: Total 808 (delta 24), reused 773 (delta 3), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (808/808), 1.79 MiB | 7.97 MiB/s, done.\n",
            "Resolving deltas: 100% (24/24), done.\n"
          ]
        }
      ],
      "source": [
        "!pwd\n",
        "# ! unzip data.zip\n",
        "! git clone https://github.com/chian-chen/BELab.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8UpdHock1gIM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from scipy import signal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ljMxzrLOgrrc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2rULqz-4ztB",
        "outputId": "34ce2843-f820-46cb-b754-87d087579150"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/BELab/data/GestureN/weng_3.npz\n",
            "/content/BELab/data/GestureN/ko_7.npz\n",
            "/content/BELab/data/GestureN/weng_2.npz\n",
            "/content/BELab/data/GestureN/weng_5.npz\n",
            "/content/BELab/data/GestureN/weng_1.npz\n",
            "/content/BELab/data/GestureN/ko_3.npz\n",
            "/content/BELab/data/GestureZ/yang_2.npz\n",
            "/content/BELab/data/GestureZ/weng_7.npz\n",
            "/content/BELab/data/GestureZ/weng_17.npz\n",
            "/content/BELab/data/GestureZ/yang_10.npz\n",
            "/content/BELab/data/GestureZ/weng_16.npz\n",
            "/content/BELab/data/GestureZ/ko_8.npz\n",
            "/content/BELab/data/GestureZ/ko_15.npz\n",
            "/content/BELab/data/GestureZ/weng_13.npz\n",
            "/content/BELab/data/GestureZ/weng_8.npz\n",
            "/content/BELab/data/GestureZ/ko_1.npz\n",
            "/content/BELab/data/GestureZ/ko_0.npz\n",
            "/content/BELab/data/GestureV/weng_9.npz\n",
            "/content/BELab/data/GestureV/ko_14.npz\n",
            "/content/BELab/data/GestureV/ko_7.npz\n",
            "/content/BELab/data/GestureV/ko_17.npz\n",
            "/content/BELab/data/GestureV/ko_10.npz\n",
            "/content/BELab/data/GestureV/ko_11.npz\n",
            "/content/BELab/data/GestureV/ko_5.npz\n",
            "/content/BELab/data/GestureV/weng_0.npz\n",
            "/content/BELab/data/GestureV/ko_4.npz\n",
            "/content/BELab/data/GestureV/ko_6.npz\n",
            "/content/BELab/data/GestureV/weng_4.npz\n",
            "/content/BELab/data/GestureO/yang_14.npz\n",
            "/content/BELab/data/GestureO/weng_3.npz\n",
            "/content/BELab/data/GestureO/yang_19.npz\n",
            "/content/BELab/data/GestureO/yang_18.npz\n",
            "/content/BELab/data/GestureO/ko_1.npz\n",
            "/content/BELab/data/GestureO/yang_17.npz\n",
            "/content/BELab/data/GestureLeft/weng_8.npz\n",
            "/content/BELab/data/Noise/ko_10.npz\n",
            "/content/BELab/data/Noise/ko_1.npz\n",
            "/content/BELab/data/Noise/ko_0.npz\n",
            "/content/BELab/data/Noise/ko_6.npz\n",
            "/content/BELab/data/GestureRight/weng_9.npz\n",
            "/content/BELab/data/GestureRight/yang_15.npz\n",
            "/content/BELab/data/GestureRight/yang_6.npz\n",
            "/content/BELab/data/GestureRight/weng_0.npz\n",
            "/content/BELab/data/GestureRight/weng_1.npz\n",
            "/content/BELab/data/GestureUp/weng_17.npz\n",
            "/content/BELab/data/GestureUp/weng_15.npz\n",
            "/content/BELab/data/GestureUp/yang_10.npz\n",
            "/content/BELab/data/GestureUp/weng_11.npz\n",
            "/content/BELab/data/GestureUp/yang_17.npz\n",
            "Total 49 bad data!\n"
          ]
        }
      ],
      "source": [
        "def normalize(signal):\n",
        "    for s in signal:\n",
        "        # s = (s-np.mean(s))/np.sqrt(np.var(s))\n",
        "        s = (s-np.mean(s))\n",
        "    return signal\n",
        "def find_max_segment_power(signal, length):\n",
        "    max_i = 0\n",
        "    max_pow = -1\n",
        "    for i in range(signal.shape[0] - length):\n",
        "        if np.square(signal[i:i+length]).sum() > max_pow:\n",
        "            max_i = i\n",
        "            max_pow = np.square(signal[i:i+length]).sum()\n",
        "            # print(max_i, max_pow)\n",
        "        # print(i, i+length, np.square(signal[i:i+length]).sum(),  max_pow)\n",
        "    return signal[max_i:max_i+length]\n",
        "\n",
        "def process_signal(signal):\n",
        "    data = np.array([\n",
        "        signal['x'], signal['rx'],  \n",
        "        signal['y'], signal['ry'], signal['z'], signal['rz'],\n",
        "    ])\n",
        "\n",
        "    length = 80\n",
        "    process_data = np.zeros((data.shape[0], length))\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        data[i] = normalize(data[i])\n",
        "        process_data[i] = find_max_segment_power(data[i], length)\n",
        "        # print(s.shape)\n",
        "    # print(process_data.shape)\n",
        "\n",
        "    return process_data\n",
        "    #return normalize(data)\n",
        "def preprocess_data(file_path):\n",
        "    X = []\n",
        "    y = []\n",
        "    num_of_bad_data = 0\n",
        "    label2digit = {\"GestureDown\":0, \"GestureLeft\":1, \"GestureN\":2,\n",
        "                   \"GestureO\": 3, \"GestureRight\":4, \"GestureUp\":5, \n",
        "                   \"GestureV\": 6, \"GestureZ\": 7, \"Noise\":8\n",
        "                   }\n",
        "    for folder in os.listdir(file_path):\n",
        "        if os.path.isdir(f\"{file_path}/{folder}\"):\n",
        "            for file in os.listdir(f\"{file_path}/{folder}\"):\n",
        "                if os.path.isfile(f\"{file_path}/{folder}/{file}\"):\n",
        "                    if file.split('.')[1] == 'npz' and file.split('.')[0].split('_')[0]!='hung':\n",
        "                        signal = np.load(f\"{file_path}/{folder}/{file}\")\n",
        "                        is_good_data = True\n",
        "                        for rx_data in signal['rx']:\n",
        "                            if rx_data > 1000 or rx_data < -1000:\n",
        "                                print(f\"{file_path}/{folder}/{file}\")\n",
        "                                num_of_bad_data += 1\n",
        "                                is_good_data = False\n",
        "                                break\n",
        "                        if is_good_data:\n",
        "                            signal = process_signal(signal)\n",
        "                            # print(signal.shape)\n",
        "                            if not np.isnan(signal).any() :\n",
        "                                X.append(np.expand_dims(signal, 0).tolist())\n",
        "                                y.append(label2digit[folder])\n",
        "                            else:\n",
        "                                num_of_bad_data += 1\n",
        "                                print(f\"{file_path}/{folder}/{file}\")\n",
        "    print(f'Total {num_of_bad_data} bad data!')\n",
        "    # print(X)\n",
        "    # return X, y\n",
        "    return torch.tensor(X), torch.tensor(y)\n",
        "X, y = preprocess_data('/content/BELab/data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "TfdK2gUyWHph"
      },
      "outputs": [],
      "source": [
        "# sig = np.load('data/GestureLeft/chen_0.npz')\n",
        "# d = process_signal(sig)\n",
        "# # print(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRQEbNyVH6GD",
        "outputId": "d191f851-5969-4e60-e9f5-a1e83174c616"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 80])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "lXHIlemZ73nL"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, signals, labels):\n",
        "        self.data = signals\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "RUuDeWjSuB3Z"
      },
      "outputs": [],
      "source": [
        "batch_size = 10\n",
        "\n",
        "my_data = MyDataset(X, y)\n",
        "\n",
        "# Define the sizes of the training and validation sets\n",
        "train_size = int(0.9 * len(my_data))\n",
        "val_size = len(my_data) - train_size\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_data, val_data = random_split(my_data, [train_size, val_size])\n",
        "\n",
        "# Create data loaders for the training and validation sets\n",
        "train_loader = DataLoader(train_data, batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feWI-sQYu4hf",
        "outputId": "e405481a-2e93-44d0-daca-71e913d9d0d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "CNN(\n",
            "  (cnn): Conv2d(1, 16, kernel_size=(1, 16), stride=(1, 1))\n",
            "  (relu): ReLU()\n",
            "  (maxpool): MaxPool2d(kernel_size=(2, 4), stride=(2, 4), padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc): Linear(in_features=768, out_features=256, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (proj_class): Linear(in_features=256, out_features=9, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "# Define model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_class, out_channel=196):\n",
        "        super().__init__()\n",
        "        self.num_class = num_class\n",
        "        self.out_channel = out_channel\n",
        "        self.cnn = nn.Conv2d(1, self.out_channel, (1, 16))\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d((2, 4), stride=(2, 4))\n",
        "        self.fc = nn.Linear(self.out_channel*3*16, 256)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.proj_class = nn.Linear(256, self.num_class)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.proj_class(x)\n",
        "        return x\n",
        "\n",
        "model = CNN(9, 16).to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqplFfcxC2f-",
        "outputId": "adcbe6c5-11c1-466c-fb8e-b80dd8a50d44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 9])\n"
          ]
        }
      ],
      "source": [
        "input = torch.randn(1, 1,6,80).to(device)\n",
        "input2 = torch.randn(1, 1,6,80).to(device)\n",
        "output = model(input2)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "wWSn_YslvPE9"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "St-l2c6RvSof"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    print(size)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # print(X)\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        # print(pred)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "kxVOLbk8vWQq"
      },
      "outputs": [],
      "source": [
        "def valid(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            # print(\"pred\")\n",
        "            # print(pred)\n",
        "            # print('y')\n",
        "            # print(y)\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    # print(size)\n",
        "    # print(acc)\n",
        "    acc = correct / size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*acc):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1OUyCmTvdw3",
        "outputId": "606cc7bd-bd18-4132-fa82-ac858b20736a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "493\n",
            "loss: 22.909239  [   10/  493]\n",
            "loss: 22.453039  [  110/  493]\n",
            "loss: 10.512362  [  210/  493]\n",
            "loss: 17.908527  [  310/  493]\n",
            "loss: 10.620424  [  410/  493]\n",
            "Test Error: \n",
            " Accuracy: 47.3%, Avg loss: 6.138021 \n",
            "\n",
            "Save better model on epoch 0\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "493\n",
            "loss: 7.518157  [   10/  493]\n",
            "loss: 9.200394  [  110/  493]\n",
            "loss: 9.641466  [  210/  493]\n",
            "loss: 8.081459  [  310/  493]\n",
            "loss: 17.994131  [  410/  493]\n",
            "Test Error: \n",
            " Accuracy: 49.1%, Avg loss: 3.427858 \n",
            "\n",
            "Save better model on epoch 1\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "493\n",
            "loss: 8.851155  [   10/  493]\n",
            "loss: 6.429410  [  110/  493]\n",
            "loss: 7.345438  [  210/  493]\n",
            "loss: 7.756417  [  310/  493]\n",
            "loss: 4.374694  [  410/  493]\n",
            "Test Error: \n",
            " Accuracy: 58.2%, Avg loss: 3.590443 \n",
            "\n",
            "Save better model on epoch 2\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "493\n",
            "loss: 5.640553  [   10/  493]\n",
            "loss: 10.224886  [  110/  493]\n",
            "loss: 7.756546  [  210/  493]\n",
            "loss: 0.470666  [  310/  493]\n",
            "loss: 4.242449  [  410/  493]\n",
            "Test Error: \n",
            " Accuracy: 69.1%, Avg loss: 2.242640 \n",
            "\n",
            "Save better model on epoch 3\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "493\n",
            "loss: 4.272574  [   10/  493]\n",
            "loss: 5.061405  [  110/  493]\n",
            "loss: 10.413298  [  210/  493]\n",
            "loss: 2.256575  [  310/  493]\n",
            "loss: 2.522802  [  410/  493]\n",
            "Test Error: \n",
            " Accuracy: 61.8%, Avg loss: 2.470654 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "493\n",
            "loss: 1.537403  [   10/  493]\n",
            "loss: 0.544516  [  110/  493]\n",
            "loss: 1.506420  [  210/  493]\n",
            "loss: 0.931939  [  310/  493]\n",
            "loss: 2.618916  [  410/  493]\n",
            "Test Error: \n",
            " Accuracy: 70.9%, Avg loss: 1.593923 \n",
            "\n",
            "Save better model on epoch 5\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "493\n",
            "loss: 3.137272  [   10/  493]\n",
            "loss: 0.260801  [  110/  493]\n",
            "loss: 11.238811  [  210/  493]\n",
            "loss: 3.957403  [  310/  493]\n",
            "loss: 6.416217  [  410/  493]\n",
            "Test Error: \n",
            " Accuracy: 72.7%, Avg loss: 1.574280 \n",
            "\n",
            "Save better model on epoch 6\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "493\n",
            "loss: 2.115152  [   10/  493]\n",
            "loss: 1.092247  [  110/  493]\n",
            "loss: 5.320001  [  210/  493]\n",
            "loss: 4.849198  [  310/  493]\n",
            "loss: 2.026478  [  410/  493]\n",
            "Test Error: \n",
            " Accuracy: 78.2%, Avg loss: 1.178173 \n",
            "\n",
            "Save better model on epoch 7\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "493\n",
            "loss: 1.408852  [   10/  493]\n",
            "loss: 1.742653  [  110/  493]\n",
            "loss: 6.770903  [  210/  493]\n",
            "loss: 1.467255  [  310/  493]\n",
            "loss: 1.048936  [  410/  493]\n",
            "Test Error: \n",
            " Accuracy: 80.0%, Avg loss: 1.073379 \n",
            "\n",
            "Save better model on epoch 8\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "493\n",
            "loss: 2.668133  [   10/  493]\n",
            "loss: 1.295070  [  110/  493]\n",
            "loss: 2.756598  [  210/  493]\n",
            "loss: 2.382722  [  310/  493]\n",
            "loss: 1.185259  [  410/  493]\n",
            "Test Error: \n",
            " Accuracy: 78.2%, Avg loss: 1.227306 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "493\n",
            "loss: 0.059574  [   10/  493]\n",
            "loss: 1.895412  [  110/  493]\n",
            "loss: 4.435784  [  210/  493]\n",
            "loss: 0.870641  [  310/  493]\n",
            "loss: 0.117657  [  410/  493]\n",
            "Test Error: \n",
            " Accuracy: 83.6%, Avg loss: 0.814774 \n",
            "\n",
            "Save better model on epoch 10\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "493\n",
            "loss: 0.522215  [   10/  493]\n",
            "loss: 5.711962  [  110/  493]\n",
            "loss: 6.205009  [  210/  493]\n",
            "loss: 3.751460  [  310/  493]\n",
            "loss: 0.540477  [  410/  493]\n",
            "Test Error: \n",
            " Accuracy: 81.8%, Avg loss: 1.116691 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "493\n",
            "loss: 1.017655  [   10/  493]\n",
            "loss: 2.527187  [  110/  493]\n",
            "loss: 3.639154  [  210/  493]\n",
            "loss: 0.456342  [  310/  493]\n",
            "loss: 2.338003  [  410/  493]\n",
            "Test Error: \n",
            " Accuracy: 74.5%, Avg loss: 1.169461 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "493\n",
            "loss: 0.378446  [   10/  493]\n",
            "loss: 0.254534  [  110/  493]\n",
            "loss: 2.575988  [  210/  493]\n",
            "loss: 0.028749  [  310/  493]\n",
            "loss: 0.040748  [  410/  493]\n",
            "Test Error: \n",
            " Accuracy: 81.8%, Avg loss: 1.186204 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "493\n",
            "loss: 0.928332  [   10/  493]\n",
            "loss: 1.117922  [  110/  493]\n",
            "loss: 0.147940  [  210/  493]\n",
            "loss: 0.477961  [  310/  493]\n",
            "loss: 4.207520  [  410/  493]\n",
            "Test Error: \n",
            " Accuracy: 80.0%, Avg loss: 1.037210 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "493\n",
            "loss: 1.232269  [   10/  493]\n",
            "loss: 0.886685  [  110/  493]\n",
            "loss: 0.439439  [  210/  493]\n",
            "loss: 0.395778  [  310/  493]\n",
            "loss: 0.081403  [  410/  493]\n",
            "Test Error: \n",
            " Accuracy: 80.0%, Avg loss: 1.131406 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "493\n",
            "loss: 0.039535  [   10/  493]\n",
            "loss: 0.288503  [  110/  493]\n",
            "loss: 0.350863  [  210/  493]\n",
            "loss: 0.368001  [  310/  493]\n",
            "loss: 0.691701  [  410/  493]\n",
            "Test Error: \n",
            " Accuracy: 80.0%, Avg loss: 1.042431 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "493\n",
            "loss: 1.861393  [   10/  493]\n",
            "loss: 1.795225  [  110/  493]\n",
            "loss: 2.121941  [  210/  493]\n",
            "loss: 0.895514  [  310/  493]\n",
            "loss: 0.803597  [  410/  493]\n",
            "Test Error: \n",
            " Accuracy: 80.0%, Avg loss: 1.113316 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "493\n",
            "loss: 1.063216  [   10/  493]\n",
            "loss: 0.605179  [  110/  493]\n",
            "loss: 0.527463  [  210/  493]\n",
            "loss: 1.412361  [  310/  493]\n",
            "loss: 2.412681  [  410/  493]\n",
            "Test Error: \n",
            " Accuracy: 81.8%, Avg loss: 1.088449 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "493\n",
            "loss: 0.756631  [   10/  493]\n",
            "loss: 2.033350  [  110/  493]\n",
            "loss: 2.816498  [  210/  493]\n",
            "loss: 0.314387  [  310/  493]\n",
            "loss: 0.636975  [  410/  493]\n",
            "Test Error: \n",
            " Accuracy: 80.0%, Avg loss: 1.239940 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 20\n",
        "best_acc = 0\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_loader, model, loss_fn, optimizer)\n",
        "    cur_acc = valid(val_loader, model, loss_fn)\n",
        "    if cur_acc > best_acc:\n",
        "        best_acc = cur_acc\n",
        "        print(f\"Save better model on epoch {t}\")\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLBJNex-RYdt",
        "outputId": "d1cae60d-1811-469b-a00c-9ba0a1635cc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "542\n",
            "[4 7 4 4 3 1 0 0 8 5 2 8 8 7 0 8 0 8 8 2 8 4 4 1 8 2 8 8 0 0 0 0 3 0]\n",
            "tensor([1, 7, 1, 7, 0, 7, 0, 5, 5, 5, 2, 0, 8, 7, 5, 0, 5, 7, 5, 2, 6, 1, 7, 1,\n",
            "        5, 2, 6, 8, 2, 6, 0, 6, 3, 3])\n",
            "[ True  True  True  True  True  True  True  True  True  True]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import PCA\n",
        "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "# clf = SVC(kernel='linear')\n",
        "print(len(my_data))\n",
        "pca = PCA(n_components=4)\n",
        "pca.fit(my_data.data.view(len(my_data), -1))\n",
        "signal_train = pca.transform(my_data.data.view(len(my_data), -1))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    signal_train, my_data.labels, test_size=5/80, random_state=42)\n",
        "\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "print(clf.predict(X_test))\n",
        "print(y_test)\n",
        "\n",
        "a = np.arange(10)\n",
        "b = np.arange(10)\n",
        "print(a == b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbU5pE1hTPM7",
        "outputId": "0c7edbe7-6119-48b7-c5c3-b3db7a2f2d2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(150, 4)\n",
            "[5.1 3.5 1.4 0.2]\n"
          ]
        }
      ],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "print(X.shape)\n",
        "print(X[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
